package com.liming.keyword;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.ling.CoreAnnotations.LemmaAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.NamedEntityTagAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.util.CoreMap;

import java.io.IOException;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class KeywordsProcess {

	private String lang = null;
	private StanfordCoreNLP pipeline = null;

	public KeywordsProcess(String lang) {
		if ("cn".equals(lang.toLowerCase()) || "chinese".equals(lang.toLowerCase())) {
			this.lang = "cn";
		} else {
			this.lang = "en";
		}
		try {
			Properties props = new Properties();
			if ("cn".equals(lang)) {
				props.load(IOUtils.readerFromString("CoreNLP-chinese.properties"));
			} else {
				props.load(IOUtils.readerFromString("CoreNLP-english.properties"));
			}
			this.pipeline = new StanfordCoreNLP(props);
		} catch (IOException e) {
			System.err.println("加载" + ("cn".equals(this.lang) ? "中文" : "英文") + "语言模型失败！\n" + e.getMessage());
			e.printStackTrace();
		}
	}
	
	public KeywordsProcess(String lang,String ner) {
		try {
			Properties props = new Properties();
			props.load(IOUtils.readerFromString("CoreNLP-english-ner.properties"));
			this.pipeline = new StanfordCoreNLP(props);
		} catch (IOException e) {
			System.err.println("加载 ner-英文语言模型失败！\n" + e.getMessage());
			e.printStackTrace();
		}
	}
	
	

	/**
	 * 提取关键词
	 * 
	 * @param documents        ：要提取关键词的文本
	 * @param lang             ：sentences的语言类型 cn - 中文 en - 英文
	 * @param wordsMin         ：最小单词长度
	 * @param wordsMax         ：最大单词长度
	 * @param stopwords        ：停用词
	 * @param alias_dictionary ：关联词
	 * @return 关键词列表
	 * @throws IOException
	 */
	public Map<String, Integer> analyseKeywords(String documents, int wordsMin, int wordsMax, Set<String> stopwords,
			Map<String, String> alias_dictionary) throws IOException {
		// 关键词
		Map<String, Integer> keywords = new HashMap<>();

		Annotation document = new Annotation(documents);
		this.pipeline.annotate(document);

		// 句子
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		// 词形
		List<String> lemmas = new ArrayList<>();
		// 词性
		List<String> poss = new ArrayList<>();

		String sentenceWithPos = "";
		for (CoreMap sentence : sentences) {
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				lemmas.add(token.get(LemmaAnnotation.class));
				poss.add(token.get(PartOfSpeechAnnotation.class));
			}
			sentenceWithPos = "";
			for (int i = 0, len = lemmas.size(); i < len; i++) {
				String key = lemmas.get(i);
				String value = poss.get(i);
				// 去除标点符号--注释调解决很多酸一类的字符串
//				String punctuationReg = "[\\,\\.\\!\\?\\:\\;\\(\\)\\[\\]\\\"\\'\\/]";
//			    Pattern punctuationPattern = Pattern.compile(punctuationReg);
//			    Matcher punctuationMatcher = punctuationPattern.matcher(value);
//				if(punctuationMatcher.matches() || "PU".equals(value)) {
//					continue;
//				}
				// 去除停用词
				if (stopwords != null && stopwords.contains(key.toLowerCase())) {
					continue;
				}
				// 替换关联词
				if (alias_dictionary != null && alias_dictionary.containsKey(key)) {
					String toAlias = alias_dictionary.get(key);
					for (int j = 0; j < len; j++) {
						if (key.equals(lemmas.get(j))) {
							lemmas.set(j, toAlias);
						}
					}
				}
				sentenceWithPos += key + "/" + value + " ";
			}
			// System.out.println(sentenceWithPos.toLowerCase());

			// 提取关键词
			String word = "(?:[\\w]+[-]*[\\w]*)";
			String posType = "N([NRPS]?)".toLowerCase();
			if ("cn".equals(lang)) {
				word = "(?:[\\u4E00-\\u9FA5]+)";
			}
			// 名词正则
			String nounPTN = word + "/" + posType;
			// wordsMin 到 wordsMax 个名词词组正则
			String nounGroupPTN = "(?:" + nounPTN + "\\s+){" + wordsMin + "," + wordsMax + "}";
			// 形容词正则
			String adjPTN = "(?:" + word + "/(JJ([RS]?)|VB([DGNPZ]?))\\s+)";
			// 关键词识别正则
			String phrasePTN = "(" + adjPTN + "*" + nounGroupPTN + ")";
			Pattern phrasePattern = Pattern.compile(phrasePTN.toLowerCase());
			Matcher phraseMatcher = phrasePattern.matcher(sentenceWithPos.toLowerCase());
			while (phraseMatcher.find()) {
				String phrase = phraseMatcher.group(1).replaceAll("/" + posType + "|/JJ([RS]?)|/VB([DGNPZ]?)".toLowerCase(), "").trim();
				if ("cn".equals(this.lang)) {
					phrase = phrase.replaceAll(" ", "");
				}
				if (keywords.containsKey(phrase)) {
					// 存在, 在原有记录上数量 + 1
					keywords.put(phrase, keywords.get(phrase) + 1);
				} else {
					// 不存在, 判断是否合并
					boolean shouldMerge = false;
					String shouldMergeKey = "";
					for (String key : keywords.keySet()) {
						if (key.contains(phrase) || phrase.contains(key)) {
							shouldMergeKey = key;
							shouldMerge = true;
						}
					}
					// 合并重复关键词
					if (!shouldMerge) {
						keywords.put(phrase, 1);
					} else {
						if (shouldMergeKey.contains(phrase)) {
							keywords.put(phrase, keywords.get(shouldMergeKey) + 1);
							keywords.remove(shouldMergeKey);
						}
						if (phrase.contains(shouldMergeKey)) {
							keywords.put(shouldMergeKey, keywords.get(shouldMergeKey) + 1);
						}
					}
				}
			}
		}
		return keywords;
	}

	public List<String> NationExtract(String txtWord) {
		List<String> nations = new ArrayList<>();
		Annotation document = new Annotation(txtWord);
		this.pipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				String ne = token.get(NamedEntityTagAnnotation.class);
				if ("COUNTRY".equals(ne)) {
					String lema = token.get(TextAnnotation.class);
					nations.add(lema);
				}
			}
		}
//		System.out.println("nation :" + StringUtils.join(nations, " "));
		return nations;

	}

	public List<String> lemmatize(String txtWord) {
		List<String> lemas = new ArrayList<>();
		Annotation document = new Annotation(txtWord);
		this.pipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		for (CoreMap sentence : sentences) {
			for (CoreLabel token : sentence.get(TokensAnnotation.class)) {
				String lema = token.get(LemmaAnnotation.class);
				lemas.add(lema);
			}
		}
//		System.out.println("nation :" + StringUtils.join(nations, " "));
		return lemas;
	}

}
